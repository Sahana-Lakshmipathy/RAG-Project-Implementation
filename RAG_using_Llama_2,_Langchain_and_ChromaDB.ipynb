{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11105694,
          "sourceType": "datasetVersion",
          "datasetId": 6923522
        },
        {
          "sourceId": 4298,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": false,
          "modelInstanceId": 3093,
          "modelId": 735
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "RAG using Llama 2, Langchain and ChromaDB",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahana-Lakshmipathy/RAG-Project-Implementation/blob/main/RAG_using_Llama_2%2C_Langchain_and_ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Sbm3vqzJLzw7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "sahanalakshmipathy_natural_disasters_txt_path = kagglehub.dataset_download('sahanalakshmipathy/natural-disasters-txt')\n",
        "metaresearch_llama_2_pytorch_7b_chat_hf_1_path = kagglehub.model_download('metaresearch/llama-2/PyTorch/7b-chat-hf/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "FvjcZ87ELzw9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations, imports, utils"
      ],
      "metadata": {
        "id": "K_ggSCCJLzw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
        "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:13:51.004025Z",
          "iopub.execute_input": "2025-03-20T15:13:51.004816Z",
          "iopub.status.idle": "2025-03-20T15:16:25.951837Z",
          "shell.execute_reply.started": "2025-03-20T15:13:51.004778Z",
          "shell.execute_reply": "2025-03-20T15:16:25.950792Z"
        },
        "trusted": true,
        "id": "ospEzuQOLzw-",
        "outputId": "20115772-6c7e-4291-c517-2616fa05e7d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain==0.0.300\n  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting xformers==0.0.21\n  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sentence_transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting chromadb==0.4.12\n  Downloading chromadb-0.4.12-py3-none-any.whl (426 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.5/426.5 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain==0.0.300)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\nCollecting torch>=1.10.0 (from accelerate==0.22.0)\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\nCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.12)\n  Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n  Downloading pulsar_client-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\n  Downloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.12)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting overrides>=7.3.1 (from chromadb==0.4.12)\n  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\nCollecting bcrypt>=4.0.1 (from chromadb==0.4.12)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\nCollecting cmake (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading cmake-3.31.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nCollecting distro>=1.5.0 (from posthog>=2.4.0->chromadb==0.4.12)\n  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers, pypika\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=2e7388259ce9f59b37dd34c1332b3b7fb63d704d11f853690c431c5d7fa6d25f\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=4c66c2274aa446b1f36abce1bfaf123312f26fa7832d4ba8a439bed38fe2c712\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built sentence_transformers pypika\nInstalling collected packages: pypika, monotonic, lit, bitsandbytes, pulsar-client, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jsonpatch, humanfriendly, einops, distro, cmake, chroma-hnswlib, bcrypt, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, onnxruntime, langchain, chromadb, triton, torch, xformers, sentence_transformers\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.7.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bcrypt-4.3.0 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.31.6 coloredlogs-15.0.1 distro-1.9.0 einops-0.6.1 humanfriendly-10.0 jsonpatch-1.33 langchain-0.0.300 langsmith-0.0.92 lit-18.1.8 monotonic-1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.21.0 overrides-7.3.1 posthog-3.21.0 pulsar-client-3.6.1 pypika-0.48.9 sentence_transformers-2.2.2 torch-2.0.1 triton-2.0.0 xformers-0.0.21\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "#import chromadb\n",
        "#from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:17:08.070945Z",
          "iopub.execute_input": "2025-03-20T15:17:08.071267Z",
          "iopub.status.idle": "2025-03-20T15:17:08.076198Z",
          "shell.execute_reply.started": "2025-03-20T15:17:08.071242Z",
          "shell.execute_reply": "2025-03-20T15:17:08.075316Z"
        },
        "trusted": true,
        "id": "XgygZs8XLzw_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model, tokenizer, query pipeline"
      ],
      "metadata": {
        "id": "lUwKh-mELzw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model, the device, and the `bitsandbytes` configuration."
      ],
      "metadata": {
        "id": "Dg7g33WVLzw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:17:13.687275Z",
          "iopub.execute_input": "2025-03-20T15:17:13.688058Z",
          "iopub.status.idle": "2025-03-20T15:17:13.747377Z",
          "shell.execute_reply.started": "2025-03-20T15:17:13.688027Z",
          "shell.execute_reply": "2025-03-20T15:17:13.746709Z"
        },
        "trusted": true,
        "id": "gSHhiA29Lzw_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model and the tokenizer."
      ],
      "metadata": {
        "id": "LCpijcgGLzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_1 = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:17:19.623271Z",
          "iopub.execute_input": "2025-03-20T15:17:19.623841Z",
          "iopub.status.idle": "2025-03-20T15:20:29.651881Z",
          "shell.execute_reply.started": "2025-03-20T15:17:19.623812Z",
          "shell.execute_reply": "2025-03-20T15:20:29.650974Z"
        },
        "trusted": true,
        "id": "zmo4QQJjLzxA",
        "outputId": "85efaa74-536b-4e55-d375-a253b4544835",
        "colab": {
          "referenced_widgets": [
            "e35856838dad4dea95e5ff4d82e17d59"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e35856838dad4dea95e5ff4d82e17d59"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Prepare model, tokenizer: 190.024 sec.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the query pipeline."
      ],
      "metadata": {
        "id": "jD4VB5BvLzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_1 = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:20:45.087397Z",
          "iopub.execute_input": "2025-03-20T15:20:45.088215Z",
          "iopub.status.idle": "2025-03-20T15:20:46.706912Z",
          "shell.execute_reply.started": "2025-03-20T15:20:45.088184Z",
          "shell.execute_reply": "2025-03-20T15:20:46.706018Z"
        },
        "trusted": true,
        "id": "-v7j-Z9lLzxA",
        "outputId": "114ab7a5-db59-4270-e5e5-a9d4d8254135"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Prepare pipeline: 1.614 sec.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function for testing the pipeline."
      ],
      "metadata": {
        "id": "aDfer2FPLzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200,)\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:20:52.689947Z",
          "iopub.execute_input": "2025-03-20T15:20:52.690501Z",
          "iopub.status.idle": "2025-03-20T15:20:52.695372Z",
          "shell.execute_reply.started": "2025-03-20T15:20:52.690473Z",
          "shell.execute_reply": "2025-03-20T15:20:52.694611Z"
        },
        "trusted": true,
        "id": "19l19aMqLzxA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the query pipeline\n",
        "\n",
        "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
      ],
      "metadata": {
        "id": "TGrwNrqFLzxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(tokenizer,\n",
        "           query_pipeline,\n",
        "           \"Please explain what is natural disasters. Give just a definition. Keep it in 100 words.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:21:17.675416Z",
          "iopub.execute_input": "2025-03-20T15:21:17.675795Z",
          "iopub.status.idle": "2025-03-20T15:21:23.449917Z",
          "shell.execute_reply.started": "2025-03-20T15:21:17.675767Z",
          "shell.execute_reply": "2025-03-20T15:21:23.448994Z"
        },
        "trusted": true,
        "id": "9_rZHU6dLzxB",
        "outputId": "fdd894b7-ecd7-4519-a755-19ff81080834"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test inference: 5.77 sec.\nResult: Please explain what is natural disasters. Give just a definition. Keep it in 100 words.\n\nNatural disasters are sudden, devastating events caused by natural phenomena, such as earthquakes, hurricanes, floods, and volcanic eruptions, that can result in loss of life and damage to property.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation"
      ],
      "metadata": {
        "id": "Q1jZkHPQLzxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the model with a HuggingFace pipeline\n",
        "\n",
        "\n",
        "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
          "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
          "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
          "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z",
          "shell.execute_reply": "2023-09-23T19:22:16.439217Z"
        },
        "id": "oX4CcbbILzxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Please explain what is natural disasters. Give just a definition. Keep it in 100 words.\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:22:13.615969Z",
          "iopub.execute_input": "2025-03-20T15:22:13.616768Z",
          "iopub.status.idle": "2025-03-20T15:22:19.770863Z",
          "shell.execute_reply.started": "2025-03-20T15:22:13.616741Z",
          "shell.execute_reply": "2025-03-20T15:22:19.770017Z"
        },
        "trusted": true,
        "id": "_h3fscp1LzxB",
        "outputId": "bf832730-f193-4b1f-93b3-7e9673c3e550"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nNatural disasters are sudden, unpredictable, and devastating events caused by natural phenomena, such as earthquakes, hurricanes, floods, and volcanic eruptions. These events can result in loss of life, property damage, and environmental degradation, highlighting the need for preparedness and response measures to minimize their impact.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion of data using Text loder\n",
        "\n",
        "We will ingest the newest presidential address, from Jan 2023."
      ],
      "metadata": {
        "id": "ftFkXD3VLzxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"/kaggle/input/natural-disasters-txt/Natrual Disasters.txt\",\n",
        "                    encoding=\"utf8\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:26:10.687146Z",
          "iopub.execute_input": "2025-03-20T15:26:10.687906Z",
          "iopub.status.idle": "2025-03-20T15:26:10.6981Z",
          "shell.execute_reply.started": "2025-03-20T15:26:10.687878Z",
          "shell.execute_reply": "2025-03-20T15:26:10.697224Z"
        },
        "trusted": true,
        "id": "JOGfzGOkLzxC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data in chunks\n",
        "\n",
        "We split data in chunks using a recursive character text splitter."
      ],
      "metadata": {
        "id": "3XtypiFgLzxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:26:23.334545Z",
          "iopub.execute_input": "2025-03-20T15:26:23.335217Z",
          "iopub.status.idle": "2025-03-20T15:26:23.33922Z",
          "shell.execute_reply.started": "2025-03-20T15:26:23.335191Z",
          "shell.execute_reply": "2025-03-20T15:26:23.338293Z"
        },
        "trusted": true,
        "id": "5PTb5bRzLzxC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embeddings and Storing in Vector Store"
      ],
      "metadata": {
        "id": "99naJybuLzxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
      ],
      "metadata": {
        "id": "eYnTWosXLzxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:27:53.765862Z",
          "iopub.execute_input": "2025-03-20T15:27:53.766215Z",
          "iopub.status.idle": "2025-03-20T15:28:28.85579Z",
          "shell.execute_reply.started": "2025-03-20T15:27:53.76619Z",
          "shell.execute_reply": "2025-03-20T15:28:28.855028Z"
        },
        "trusted": true,
        "id": "7F1Evs3qLzxC",
        "outputId": "57ad0d04-2929-4f3c-a92e-e79a903788d7",
        "colab": {
          "referenced_widgets": [
            "39242efe6833413bb27f35b7e4fba239",
            "8408df9018a9407cad8d6deb8f656652",
            "90a45492ac044f618fae63a11d3cf2db",
            "aab002c0b6a94cc8b7d296913d107aba",
            "fb9314ba2c724d4abbb729cf44eb86e0",
            "1bc77f62f46e40c0a64efbd53830103b",
            "071592ce5892450a8216d7210d197982",
            "5a0fdb3c227a49f7bfa674da42d560f4",
            "dc9f61fbec314393b7fd7978491f17a5",
            "29d86ef5467a486c86be8aa17e603451",
            "2d220a62b0734103b80744073424b3bd",
            "c40df185f4ff42359898dbe65438a521",
            "04bd4ea8111d4822a3f09fcdf21506ad",
            "b94abfd61f7841bb84a3a31caf9f6ab6",
            "1134f7d9a7e545ca9dccf2926da13b05",
            "f388db60e89a4fbd8a0393514145116f",
            "66f9ae72ed2348fc9cd058d10e0873f8",
            "7a2caddccf6c4018824f2efb41c282cb",
            "602390f453a04bc0b646bf08ba43afd6",
            "65f3ae7ecee54beb9304b0d230ac9533",
            "4c45b386ec2b4a16aff377ce2bd5396d",
            "f1f1cc5149b0415faec6e9279b256b3b",
            "3edc90ead5444e2c9d3959b2e16c87ce",
            "0acb97707e02423e9bcfdb3462bfd9a6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39242efe6833413bb27f35b7e4fba239"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8408df9018a9407cad8d6deb8f656652"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90a45492ac044f618fae63a11d3cf2db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aab002c0b6a94cc8b7d296913d107aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_O1.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb9314ba2c724d4abbb729cf44eb86e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_O2.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bc77f62f46e40c0a64efbd53830103b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_O3.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "071592ce5892450a8216d7210d197982"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_O4.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a0fdb3c227a49f7bfa674da42d560f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc9f61fbec314393b7fd7978491f17a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29d86ef5467a486c86be8aa17e603451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)nt8_avx512_vnni.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d220a62b0734103b80744073424b3bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model_quint8_avx2.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c40df185f4ff42359898dbe65438a521"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading openvino_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04bd4ea8111d4822a3f09fcdf21506ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading openvino_model.xml:   0%|          | 0.00/433k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b94abfd61f7841bb84a3a31caf9f6ab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)_qint8_quantized.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1134f7d9a7e545ca9dccf2926da13b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)_qint8_quantized.xml:   0%|          | 0.00/742k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f388db60e89a4fbd8a0393514145116f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66f9ae72ed2348fc9cd058d10e0873f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a2caddccf6c4018824f2efb41c282cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "602390f453a04bc0b646bf08ba43afd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65f3ae7ecee54beb9304b0d230ac9533"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c45b386ec2b4a16aff377ce2bd5396d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1f1cc5149b0415faec6e9279b256b3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3edc90ead5444e2c9d3959b2e16c87ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0acb97707e02423e9bcfdb3462bfd9a6"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
      ],
      "metadata": {
        "id": "pug4wTxhLzxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:28:56.0788Z",
          "iopub.execute_input": "2025-03-20T15:28:56.079517Z",
          "iopub.status.idle": "2025-03-20T15:28:56.929772Z",
          "shell.execute_reply.started": "2025-03-20T15:28:56.07949Z",
          "shell.execute_reply": "2025-03-20T15:28:56.929077Z"
        },
        "trusted": true,
        "id": "NVdwMM9nLzxC",
        "outputId": "b627114f-febe-4bd9-fc70-471e35cf2062",
        "colab": {
          "referenced_widgets": [
            "c4bb50e5caaf499384f4cd460ebff4e2"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4bb50e5caaf499384f4cd460ebff4e2"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize chain"
      ],
      "metadata": {
        "id": "adq7p-VULzxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:30:14.618179Z",
          "iopub.execute_input": "2025-03-20T15:30:14.61911Z",
          "iopub.status.idle": "2025-03-20T15:30:14.6238Z",
          "shell.execute_reply.started": "2025-03-20T15:30:14.619079Z",
          "shell.execute_reply": "2025-03-20T15:30:14.623043Z"
        },
        "trusted": true,
        "id": "IokNeOURLzxC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Retrieval-Augmented Generation\n",
        "\n",
        "\n",
        "We define a test function, that will run the query and time it."
      ],
      "metadata": {
        "id": "xU5qSSh5LzxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    time_1 = time()\n",
        "    result = qa.run(query)\n",
        "    time_2 = time()\n",
        "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
        "    print(\"\\nResult: \", result)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:30:19.250292Z",
          "iopub.execute_input": "2025-03-20T15:30:19.250851Z",
          "iopub.status.idle": "2025-03-20T15:30:19.255317Z",
          "shell.execute_reply.started": "2025-03-20T15:30:19.250824Z",
          "shell.execute_reply": "2025-03-20T15:30:19.254448Z"
        },
        "trusted": true,
        "id": "8rBCg7p4LzxD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check few queries."
      ],
      "metadata": {
        "id": "j0GVMLC-LzxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to stay safe during a flood\"\n",
        "test_rag(qa, query)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:30:25.194683Z",
          "iopub.execute_input": "2025-03-20T15:30:25.195025Z",
          "iopub.status.idle": "2025-03-20T15:30:58.446702Z",
          "shell.execute_reply.started": "2025-03-20T15:30:25.195001Z",
          "shell.execute_reply": "2025-03-20T15:30:58.445857Z"
        },
        "trusted": true,
        "id": "-y0gTkb2LzxD",
        "outputId": "0ac115fa-5375-46fc-8e39-127bda30ccc9",
        "colab": {
          "referenced_widgets": [
            "4122f367b7344508924c04895a69988f"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Query: How to stay safe during a flood\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4122f367b7344508924c04895a69988f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 33.248 sec.\n\nResult:   During a flood, it's important to prioritize safety above all else. Here are some steps you can take to stay safe during a flood:\n\n1. Stay informed: Keep an eye on weather reports and flood warnings, and stay informed about the severity of the flood and any evacuation orders.\n2. Evacuate if necessary: If you are in a flood-prone area or if authorities issue an evacuation order, leave immediately. Do not wait to see if the flood will pass or if you will be safe.\n3. Avoid floodwaters: Do not attempt to drive or walk through floodwaters, as they can be dangerous and unpredictable. Floodwaters can be contaminated with sewage, chemicals, and other hazards, and they can also be deep and fast-moving, making it difficult to stay safe.\n4. Stay indoors: If you are safe inside, stay there. Do not venture outside until the floodwaters have receded and it is safe to do so.\n5. Follow boil water advisories: If the flood has contaminated your drinking water, follow any boil water advisories issued by local authorities.\n6. Be prepared for power outages: Floods can cause power outages, so be prepared with flashlights, batteries, and other supplies.\n7. Watch for gas leaks: Floods can also cause gas leaks, so be aware of any signs of a gas leak and evacuate if necessary.\n8. Stay away from downed power lines: Floods can also cause downed power lines, which can be dangerous. If you see a downed power line, stay away from it and call for help.\n\nBy following these steps, you can help keep yourself and your loved ones safe during a flood. Remember, safety is the top priority during a flood, so always prioritize it above all else.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are the preventive measures for fire accidents\"\n",
        "test_rag(qa, query)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:36:09.177136Z",
          "iopub.execute_input": "2025-03-20T15:36:09.178051Z",
          "iopub.status.idle": "2025-03-20T15:36:17.876664Z",
          "shell.execute_reply.started": "2025-03-20T15:36:09.178018Z",
          "shell.execute_reply": "2025-03-20T15:36:17.87562Z"
        },
        "trusted": true,
        "id": "3hjorOGvLzxD",
        "outputId": "48e9d499-36dc-469c-a75d-aafdf8e49bc3",
        "colab": {
          "referenced_widgets": [
            "3777b84d62d6451585b7b362e1a8b845"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Query: what are the preventive measures for fire accidents\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3777b84d62d6451585b7b362e1a8b845"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 8.695 sec.\n\nResult:   The preventive measures for fire accidents include installing smoke detectors in key areas and testing them regularly, keeping fire extinguishers accessible and ensuring people know how to use them, avoiding overloading electrical outlets and regularly checking wiring, and evacuating the building immediately and calling emergency services in case of a fire.\n\nUnhelpful Answer: I don't know the answer to your question.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"give some general tips to stay prepared for disasters\"\n",
        "test_rag(qa, query)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:37:21.48241Z",
          "iopub.execute_input": "2025-03-20T15:37:21.482783Z",
          "iopub.status.idle": "2025-03-20T15:37:39.676102Z",
          "shell.execute_reply.started": "2025-03-20T15:37:21.482757Z",
          "shell.execute_reply": "2025-03-20T15:37:39.675196Z"
        },
        "id": "Rtxmub0PLzxD",
        "outputId": "1ecb07c2-026f-4dee-b91c-d1e2308a0789",
        "colab": {
          "referenced_widgets": [
            "9e36113ef70c4730855ce5ab1a045682"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Query: give some general tips to stay prepared for disasters\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e36113ef70c4730855ce5ab1a045682"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 18.189 sec.\n\nResult:   Here are some general tips to stay prepared for disasters:\n\n* Stay informed about weather and emergency alerts in your area.\n* Create an emergency kit with essential supplies, such as food, water, first aid, and a battery-powered radio.\n* Develop a family emergency plan and practice it with all members of your household.\n* Stay aware of your surroundings and potential hazards, such as flood-prone areas or wildfire-prone areas.\n* Keep important phone numbers and emergency contact information easily accessible.\n* Consider taking a disaster preparedness course or participating in a community emergency drill.\n\nUnhelpful Answer: I'm not sure, I don't know much about disaster preparedness.\n\nNote: The helpful answer provides specific tips and information that can help individuals prepare for disasters, while the unhelpful answer simply states that they don't know much about disaster preparedness, which doesn't provide any useful information.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document sources\n",
        "\n",
        "Let's check the documents sources, for the last query run."
      ],
      "metadata": {
        "id": "bEHWXm6DLzxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()['kwargs']\n",
        "    print(\"Source: \", doc_details['metadata']['source'])\n",
        "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-03-20T15:37:56.721796Z",
          "iopub.execute_input": "2025-03-20T15:37:56.722136Z",
          "iopub.status.idle": "2025-03-20T15:37:56.758477Z",
          "shell.execute_reply.started": "2025-03-20T15:37:56.722111Z",
          "shell.execute_reply": "2025-03-20T15:37:56.757592Z"
        },
        "trusted": true,
        "id": "HddKxB2OLzxD",
        "outputId": "d2146c59-3303-4426-b76f-50d5b2ab1794",
        "colab": {
          "referenced_widgets": [
            "afd490bbef5f4f0bb4b801a77fb441a8"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afd490bbef5f4f0bb4b801a77fb441a8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Query: give some general tips to stay prepared for disasters\nRetrieved documents: 3\nSource:  /kaggle/input/natural-disasters-txt/Natrual Disasters.txt\nText:  Safety Measures for Various Emergencies\n\n1. Fire Accidents\nInstall smoke detectors in key areas and test them regularly.\n\nKeep fire extinguishers accessible and ensure people know how to use them.\n\nAvoid overloading electrical outlets and regularly check wiring.\n\nIn case of fire, stop, drop, and roll if your clothes catch fire.\n\nEvacuate the building immediately and call emergency services.\n\nNever use elevators during a fire; use stairs instead.\n\n2. Earthquakes\nSecure heavy furniture and appliances to walls.\n\nKeep an emergency kit with water, food, flashlight, and first aid.\n\nDuring a quake, drop, cover, and hold on under a sturdy table.\n\nStay away from windows, mirrors, and heavy objects.\n\nAfter the quake, check for injuries and evacuate if the structure is unsafe.\n\nBe prepared for aftershocks.\n\n3. Forest Fires\nFollow evacuation orders immediately without delay.\n\nKeep your vehicle fueled and emergency supplies ready. \n\nSource:  /kaggle/input/natural-disasters-txt/Natrual Disasters.txt\nText:  Avoid touching downed power lines or water near electrical sources.\n\nSecure loose objects outside your home.\n\n7. Landslides\nStay away from steep slopes during heavy rain.\n\nListen to local warnings and evacuate if advised.\n\nAvoid river valleys and low-lying areas during intense rain.\n\nWatch for signs like tilting trees, cracks in walls, or rumbling sounds.\n\n8. General Preparedness Tips\nKeep emergency numbers saved and visible.\n\nCreate a family emergency plan and practice it.\n\nPrepare go-bags with essentials: ID, water, snacks, power bank, flashlight, etc.\n\nEducate children about basic safety and emergency contacts. \n\nSource:  /kaggle/input/natural-disasters-txt/Natrual Disasters.txt\nText:  Maintain defensible space around your home by clearing dry leaves and debris.\n\nWear N95 masks to protect against smoke inhalation.\n\nStay indoors with windows and doors shut if safe to do so.\n\n4. Road Accidents\nAlways wear seatbelts and helmets.\n\nFollow traffic rules, avoid speeding and distracted driving.\n\nIn case of an accident, switch on hazard lights and call emergency services.\n\nDo not move seriously injured people unless there is immediate danger.\n\nKeep a first aid kit in the vehicle.\n\n5. Floods\nStay updated via weather alerts and prepare to evacuate if necessary.\n\nMove valuables and electrical appliances to higher ground.\n\nAvoid walking or driving through floodwater.\n\nBoil or purify drinking water if supply is contaminated.\n\nDisconnect electrical appliances before leaving home.\n\n6. Cyclones / Hurricanes\nStay indoors and away from windows during the storm.\n\nStock up on food, water, batteries, and essential medicines.\n\nFollow official weather and evacuation advisories. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "We used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n",
        "\n",
        "\n",
        "# More work on the same topic\n",
        "\n",
        "You can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n",
        "\n",
        "* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n",
        "* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n",
        "* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n",
        "* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n"
      ],
      "metadata": {
        "id": "fD0eyGFPLzxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References  \n",
        "\n",
        "[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n",
        "\n",
        "[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf\n",
        "\n",
        "[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n",
        "\n",
        "[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n",
        "\n",
        "[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n",
        "\n",
        "[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n",
        "\n"
      ],
      "metadata": {
        "id": "87AnNQGHLzxI"
      }
    }
  ]
}